{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import jieba\n",
    "input_words = []\n",
    "f = open('5.5w_vector.txt','r',encoding= 'utf-8')\n",
    "content = f.read().split()\n",
    "aftersplit = []\n",
    "for i in content:\n",
    "    text_cut = jieba.cut(i)\n",
    "    string = ' '.join(text_cut)\n",
    "    aftersplit.append(string)\n",
    "    str_list = string.split(' ')\n",
    "    for j in str_list:\n",
    "        input_words.append(j)\n",
    "    \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "word2int = {}\n",
    "set_words = set(input_words)\n",
    "\n",
    "for i,word in enumerate(set_words):\n",
    "    word2int[word] = i\n",
    "    \n",
    "int2word = dict([(value,key) for key, value in word2int.items()])\n",
    "    \n",
    "sentences = []\n",
    "for sentence in aftersplit:\n",
    "    sentences.append(sentence.split())\n",
    "    \n",
    "WINDOW_SIZE = 2\n",
    "data = []\n",
    "for sentence in sentences:\n",
    "    for idx, word in enumerate(sentence):\n",
    "        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            if neighbor != word:\n",
    "                data.append([word, neighbor])\n",
    "\n",
    "                \n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        data[i][j]=word2int[data[i][j]]\n",
    "\n",
    "\n",
    "def get_batch_data(data,batch_size):\n",
    "    center_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    random_index = np.random.randint(0,len(data),batch_size)\n",
    "    for i in random_index:\n",
    "        center_batch.append(data[i][0])\n",
    "        target_batch.append(data[i][1])\n",
    "    \n",
    "    \n",
    "    center_batch = np.array(center_batch)\n",
    "    target_batch = np.reshape(np.array(target_batch),[batch_size,1])\n",
    "    \n",
    "    return center_batch, target_batch\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17467"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class skipgram(object):\n",
    "    def __init__(self,vocab_size, batch_size, embed_size, num_sampled, learning_rate,sess):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.lr = learning_rate\n",
    "        self.build_model()\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        self.sess = sess\n",
    "        self.chkpt_dir = os.getcwd()\n",
    "        self.checkpoint_file = os.path.join(self.chkpt_dir,'nlpwork', 'SKIPGRAM.ckpt')\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope('skipgram'):\n",
    "            self.center_words = tf.placeholder(tf.int32, shape=[None], name=\"center_words\")\n",
    "            self.target_words = tf.placeholder(tf.int32, shape=[None, 1], name=\"taget_words\")\n",
    "            self.embedding_matrix = tf.Variable(tf.random_uniform([self.vocab_size, self.embed_size], -1.0, 1.0), \n",
    "                                                name=\"embedding_matrix\")\n",
    "            \n",
    "            self.embed = tf.nn.embedding_lookup(self.embedding_matrix, self.center_words, name='embed')\n",
    "            nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size],\n",
    "                                                        stddev=1.0 / (self.embed_size ** 0.5)), \n",
    "                                                        name='nce_weight')\n",
    "            nce_bias = tf.Variable(tf.zeros([self.vocab_size]), name='nce_bias')\n",
    "\n",
    "            self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                                        biases=nce_bias, \n",
    "                                                        labels=self.target_words, \n",
    "                                                        inputs=self.embed, \n",
    "                                                        num_sampled=self.num_sampled, \n",
    "                                                        num_classes=self.vocab_size), name='loss')\n",
    "            \n",
    "            self.norm = tf.sqrt(tf.reduce_sum(tf.square(self.embedding_matrix), 1, keep_dims = True))\n",
    "            \n",
    "\n",
    "    def train(self,inputs,outputs):\n",
    "        return self.sess.run(self.optimizer, feed_dict = {self.center_words:inputs, self.target_words:outputs})\n",
    "    \n",
    "    def save_models(self):\n",
    "        print(\"...Saving checkpoint...\")\n",
    "        self.saver.save(self.sess, self.checkpoint_file)\n",
    "        \n",
    "    def load_models(self):\n",
    "        print(\"...Loading checkpoint...\")\n",
    "        self.saver.restore(self.sess, self.checkpoint_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,session):\n",
    "    model.load_models()\n",
    "    test_index = np.random.randint(0,len(set_words),8)\n",
    "    test_words = []\n",
    "    \n",
    "    norm = session.run(model.norm)\n",
    "    matrix = session.run(model.embedding_matrix)\n",
    "    normalized_mat = matrix/norm\n",
    "    \n",
    "    \n",
    "    for i in test_index:\n",
    "        test_words.append(list(set_words)[i])\n",
    "        \n",
    "    for i in test_words:\n",
    "        int_ = word2int[i]\n",
    "        int_ = np.array(int_)\n",
    "        int_ = np.reshape(int_,[1])\n",
    "        predicted = session.run(model.embed,feed_dict = {model.center_words:int_})\n",
    "        similarity = np.matmul(predicted, np.transpose(normalized_mat))\n",
    "        c = (-similarity).argsort()[0:8]\n",
    "        for j in range(8):\n",
    "            close_word = int2word[list(c[0])[j]]\n",
    "            print(i,close_word)\n",
    "    \n",
    "    np.savetxt('embedding.txt',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244.38812\n",
      "...Saving checkpoint...\n",
      "...Loading checkpoint...\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\18056\\nlpwork\\SKIPGRAM.ckpt\n",
      "羊头 羊头\n",
      "羊头 高团村\n",
      "羊头 报装\n",
      "羊头 楼房\n",
      "羊头 A0181463\n",
      "羊头 崩\n",
      "羊头 道口\n",
      "羊头 帐号\n",
      "偷 偷\n",
      "偷 孙师傅\n",
      "偷 孟哲\n",
      "偷 未尝不可\n",
      "偷 交张琦\n",
      "偷 残疾证\n",
      "偷 这部分\n",
      "偷 孝华青\n",
      "青州 青州\n",
      "青州 七天\n",
      "青州 东登村\n",
      "青州 街虞河\n",
      "青州 蓝石\n",
      "青州 快些\n",
      "青州 上线\n",
      "青州 袁崇兵\n",
      "打开 打开\n",
      "打开 加网\n",
      "打开 天算\n",
      "打开 共交\n",
      "打开 排号\n",
      "打开 进山\n",
      "打开 二级\n",
      "打开 打工\n",
      "山久 山久\n",
      "山久 卫生室\n",
      "山久 己交\n",
      "山久 没人到\n",
      "山久 续元\n",
      "山久 沂\n",
      "山久 留得\n",
      "山久 结果\n",
      "跟着 跟着\n",
      "跟着 据说\n",
      "跟着 不弄\n",
      "跟着 3568\n",
      "跟着 敢\n",
      "跟着 许商\n",
      "跟着 算是\n",
      "跟着 徼\n",
      "回拨 回拨\n",
      "回拨 不明\n",
      "回拨 港办\n",
      "回拨 葡萄\n",
      "回拨 中天\n",
      "回拨 连上\n",
      "回拨 违约金\n",
      "回拨 乡镇\n",
      "右边 右边\n",
      "右边 王府\n",
      "右边 相应\n",
      "右边 个旧\n",
      "右边 甜甜\n",
      "右边 室户\n",
      "右边 想着\n",
      "右边 话\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    session = tf.Session()\n",
    "    VOCAB_SIZE = 50000\n",
    "    BATCH_SIZE = 128\n",
    "    EMBED_SIZE = 128 # Dimention of word embedding vector\n",
    "    SKIP_WINDOW = 1 # The context window\n",
    "    NUM_SAMPLED = 64 # Number of negative examples to sample\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_TRAINING_STEP = 1\n",
    "    SKIP_STEP = 2000\n",
    "    \n",
    "    \n",
    "\n",
    "    model = skipgram(len(set_words),BATCH_SIZE,EMBED_SIZE,NUM_SAMPLED,LEARNING_RATE,session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(NUM_TRAINING_STEP):\n",
    "        centers, targets = get_batch_data(data,BATCH_SIZE)\n",
    "\n",
    "        \n",
    "        model.train(centers, targets)\n",
    "        print(session.run(model.loss, feed_dict = {model.center_words:centers, model.target_words:targets}))\n",
    "    \n",
    "    model.save_models()\n",
    "    \n",
    "        \n",
    "    \n",
    "    test(model,session)\n",
    "    \n",
    "        \n",
    "\n",
    "main()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
